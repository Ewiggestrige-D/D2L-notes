# /home/d2l/d2l/myd2l/torch.py
#æœ¬åŒ…ç”¨äºd2lå­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¸ªäººè°ƒè¯•ä¸æ›´æ–°ï¼Œç”¨äºé‡æ„d2l/torch.py
#è¿˜åŒ…å«/home/d2l/d2l/setup.py &  
# /home/d2l/d2l/pyproject.toml & 
# /home/d2l/d2l/myd2l.egg-info ç”¨äºæ„å»ºæœ¬åœ°pkg

#ä»£ç æ›´æ–°å,åœ¨ipykernelè¿è¡Œæ—¶éœ€è¦é‡å¯kernel

import numpy as np
import torch
import torchvision
from PIL import Image
from torch import nn
from torch.nn import functional as F
from torch.utils import data
from torchvision import transforms

nn_Module = nn.Module

#################   WARNING   ################
# The below part is generated automatically through:
#    d2lbook build lib
# Don't edit it directly

import collections
import hashlib
import math
import os
import random
import re
import shutil
import sys
import tarfile
import time
import zipfile
from collections import defaultdict
import pandas as pd
import requests
from IPython import display
from matplotlib import pyplot as plt
from matplotlib_inline import backend_inline

"""
å‡¡æ˜¯ï¼š

ä¸å‚ä¸å‰å‘ / åå‘ä¼ æ’­

ä¸è¿›å…¥è®­ç»ƒå¾ªç¯ hot path

åªæ˜¯ I/Oã€å·¥å…·ã€è®¾å¤‡é€‰æ‹©

ğŸ‘‰ ä¸éœ€è¦æœ¬åœ°åŒ–ï¼Œç›´æ¥ç”¨å³å¯
"""

def use_svg_display():
    """ä½¿ç”¨svgæ ¼å¼åœ¨Jupyterä¸­æ˜¾ç¤ºç»˜å›¾
    è®© Jupyter Notebook ä¸­çš„ Matplotlib å›¾å½¢ä»¥ SVGï¼ˆçŸ¢é‡å›¾ï¼‰æ ¼å¼ æ˜¾ç¤ºï¼Œè€Œéé»˜è®¤çš„ PNGã€‚
    backend_inline æ˜¯ matplotlib-inline åŒ…æä¾›çš„æ¨¡å—ï¼ˆJupyter å†…è”åç«¯ï¼‰ã€‚
    æ­¤è°ƒç”¨å‘Šè¯‰ Jupyterï¼šâ€œä»¥åæ‰€æœ‰ matplotlib å›¾éƒ½ç”¨ SVG æ ¼å¼æ¸²æŸ“â€ã€‚
    ğŸ’¡ æ³¨æ„ï¼šæ­¤è®¾ç½®å¯¹å½“å‰ notebook ä¼šè¯å…¨å±€ç”Ÿæ•ˆã€‚

    Defined in :numref:`sec_calculus`"""
    backend_inline.set_matplotlib_formats('svg') 

def set_figsize(figsize=(3.5, 2.5)):
    """è®¾ç½®matplotlibçš„å›¾è¡¨å¤§å°
    plt.rcParams æ˜¯ Matplotlib çš„å…¨å±€é…ç½®å­—å…¸ã€‚
    'figure.figsize' æ§åˆ¶å›¾å½¢é»˜è®¤å®½é«˜ï¼ˆå•ä½ï¼šè‹±å¯¸ï¼‰ã€‚

    Defined in :numref:`sec_calculus`"""
    use_svg_display()
    plt.rcParams['figure.figsize'] = figsize

def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """è®¾ç½®matplotlibçš„è½´
    
    Defined in :numref:`sec_calculus`"""
    axes.set_xlabel(xlabel) # è®¾ç½® x è½´æ ‡ç­¾ã€‚
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale) # è®¾ç½® x è½´åˆ»åº¦ç±»å‹ï¼ˆçº¿æ€§/å¯¹æ•°ç­‰ï¼‰
    axes.set_yscale(yscale)
    axes.set_xlim(xlim) # è®¾ç½® x è½´æ˜¾ç¤ºèŒƒå›´ï¼ˆå¿…é¡»æ˜¯äºŒå…ƒ tuple/listï¼‰ã€‚
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend) # å¦‚æœ legend éç©ºï¼ˆä¸æ˜¯ None æˆ–ç©ºåˆ—è¡¨ï¼‰ï¼Œåˆ™æ˜¾ç¤ºå›¾ä¾‹ã€‚
    axes.grid() #æ˜¾ç¤ºç½‘æ ¼çº¿ï¼Œä¾¿äºè¯»å›¾ã€‚ 


def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(8, 4.5), axes=None):
    """ç»˜åˆ¶æ•°æ®ç‚¹
    x åæ ‡æ•°æ®ï¼ˆå¯ä¸º listã€tensorã€ndarrayï¼‰
    y åæ ‡æ•°æ®ï¼›è‹¥ä¸º Noneï¼Œåˆ™ X è¢«å½“ä½œ y å€¼ï¼Œx è‡ªåŠ¨ä¸ºç´¢å¼•
    fmts: æ¯æ¡çº¿çš„æ ·å¼ï¼ˆé¢œè‰²+çº¿å‹ï¼‰,é»˜è®¤å€¼ ('-', 'm--', ...)

    Defined in :numref:`sec_calculus`"""
    if legend is None:
        legend = []

    set_figsize(figsize)
    axes = axes if axes else plt.gca()

    # å¦‚æœXæœ‰ä¸€ä¸ªè½´ï¼Œè¾“å‡ºTrueï¼Œ åˆ¤æ–­ X æ˜¯å¦æ˜¯â€œä¸€ç»´æ•°æ®â€ã€‚
    def has_one_axis(X):
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y): # å¦‚æœ X åªæœ‰ä¸€ç»„ï¼Œä½† Y æœ‰å¤šç»„ï¼ˆå¸¸è§æƒ…å†µï¼‰ï¼Œåˆ™å¤åˆ¶ X ä½¿å…¶åŒ¹é… Y çš„æ•°é‡ã€‚
        X = X * len(Y)
    axes.cla() # æ¸…ç©ºå½“å‰åæ ‡è½´ï¼Œé¿å…æ—§å›¾æ®‹ç•™ã€‚
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


class Timer:
    """è®°å½•å¤šæ¬¡è¿è¡Œæ—¶é—´"""
    def __init__(self):
        """Defined in :numref:`subsec_linear_model`"""
        self.times = []
        self.start()

    def start(self):
        """å¯åŠ¨è®¡æ—¶å™¨"""
        self.tik = time.time()

    def stop(self):
        """åœæ­¢è®¡æ—¶å™¨å¹¶å°†æ—¶é—´è®°å½•åœ¨åˆ—è¡¨ä¸­"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """è¿”å›å¹³å‡æ—¶é—´"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """è¿”å›æ—¶é—´æ€»å’Œ"""
        return sum(self.times)

    def cumsum(self):
        """è¿”å›ç´¯è®¡æ—¶é—´"""
        return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
    """ç”Ÿæˆy=Xw+b+å™ªå£° é€ ä¸€ä¸ªâ€œå¯æ§çš„æ•°æ®é›†â€
    w: çœŸå®æƒé‡ï¼ˆtensorï¼Œå½¢çŠ¶ (d,)ï¼‰
    b: çœŸå®åç½®ï¼ˆæ ‡é‡æˆ– 0-d tensorï¼‰
    num_examples: æ ·æœ¬æ•°

    Defined in :numref:`sec_linear_scratch`"""
    X = torch.normal(0, 1, (num_examples, len(w))) # æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª len(w) ç»´å‘é‡ï¼Œç‰¹å¾æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1)) 

def linreg(X, w, b):
    """çº¿æ€§å›å½’æ¨¡å‹

    Defined in :numref:`sec_linear_scratch`"""
    return torch.matmul(X, w) + b

def squared_loss(y_hat, y):
    """å‡æ–¹æŸå¤±
    reshapeï¼š ç¡®ä¿y å’Œ y_hat å½¢çŠ¶ä¸€è‡´ï¼Œé¿å…å¹¿æ’­é”™è¯¯
    Defined in :numref:`sec_linear_scratch`"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  

def sgd(params, lr, batch_size):
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
    params: [w, b]ï¼ˆtensor åˆ—è¡¨ï¼‰
    lr: å­¦ä¹ ç‡
    batch_size: æ‰¹å¤§å°

    Defined in :numref:`sec_linear_scratch`"""
    with torch.no_grad(): # å‘Šè¯‰ autogradï¼šä¸‹é¢çš„æ“ä½œä¸è¦å»ºè®¡ç®—å›¾
        for param in params:
            param -= lr * param.grad / batch_size # ç´¯ç§¯çš„æ¢¯åº¦/ æ‰¹å¤§å° = å¹³å‡æ¢¯åº¦
            param.grad.zero_() #  å¿…é¡»æœ‰ï¼PyTorch é»˜è®¤æ¢¯åº¦æ˜¯ ç´¯ç§¯çš„

def load_array(data_arrays, batch_size, is_train=True):
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨
    torch.utils.data.TensorDataset ï¼šæ˜¯ä¸€ä¸ªæ•°æ®é›†åŒ…è£…å™¨ï¼Œç”¨äºå°†å¤šä¸ªå¼ é‡æ‰“åŒ…æˆä¸€ä¸ªæ•°æ®é›†ï¼Œè¦æ±‚æ‰€æœ‰å¼ é‡çš„ç¬¬ä¸€ç»´ï¼ˆæ ·æœ¬æ•°ï¼‰ç›¸åŒã€‚
                                    æ¯æ¬¡ç´¢å¼• dataset[i] è¿”å› (tensor1[i], tensor2[i], ..., tensorN[i])
                                    å¸¸ç”¨äºï¼š(X, y) é…å¯¹ï¼Œå³ç‰¹å¾å’Œæ ‡ç­¾ä¸€èµ·è¿­ä»£
    e.g.: 
    X = torch.randn(100, 2)   # 100 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾
    y = torch.randn(100, 1)   # 100 ä¸ªæ ‡ç­¾

    dataset = TensorDataset(X, y)
    print(dataset[0])  # è¾“å‡º: (tensor([x1, x2]), tensor([y1]))

    * æ˜¯ Python çš„ â€œè§£åŒ…æ“ä½œç¬¦â€ï¼ˆunpacking operatorï¼‰ å®ƒæŠŠä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å…ƒç»„ï¼‰å±•å¼€ä¸ºå¤šä¸ªç‹¬ç«‹å‚æ•°ã€‚

    data_arrays = (features, labels)  # ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
    # ä¸ä½¿ç”¨ *ï¼š
    TensorDataset(data_arrays)        # âŒ é”™è¯¯ï¼ä¼ å…¥çš„æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œä¸æ˜¯ä¸¤ä¸ªå¼ é‡

    # ä½¿ç”¨ *ï¼š
    TensorDataset(*data_arrays)       # âœ… ç­‰ä»·äº TensorDataset(features, labels)
    
    Defined in :numref:`sec_linear_concise`"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

def get_fashion_mnist_labels(labels):
    """è¿”å›Fashion-MNISTæ•°æ®é›†çš„æ–‡æœ¬æ ‡ç­¾

    Defined in :numref:`sec_fashion_mnist`"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    """ç»˜åˆ¶å›¾åƒåˆ—è¡¨

    Defined in :numref:`sec_fashion_mnist`"""
    figsize = (num_cols * scale, num_rows * scale)
    #å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ fig å¯¹è±¡ï¼ˆä¸è¿›è¡Œä¿å­˜ã€è°ƒæ•´æ•´ä½“å¸ƒå±€ç­‰æ“ä½œï¼‰
    #ç”¨ _ æ˜¯ Python æƒ¯ä¾‹ï¼Œè¡¨ç¤ºâ€œå¿½ç•¥è¿™ä¸ªå˜é‡â€
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    # å°† 2D å­å›¾æ•°ç»„è½¬ä¸º 1Dï¼Œä¾¿äºé¡ºåºéå†
    axes = axes.flatten()
    #zip(axes, imgs) å°† axesï¼ˆå­å›¾åˆ—è¡¨ï¼‰å’Œ imgsï¼ˆå›¾åƒåˆ—è¡¨ï¼‰é…å¯¹,ç”Ÿæˆè¿­ä»£å™¨ï¼š(axes[0], imgs[0]), (axes[1], imgs[1])
    #enumerate(...) ç»™æ¯ä¸ªé…å¯¹åŠ ä¸Šç´¢å¼• iï¼š(0, (ax0, img0)), (1, (ax1, img1)), ...
    for i, (ax, img) in enumerate(zip(axes, imgs)): 
        if torch.is_tensor(img):
            # å›¾ç‰‡å¼ é‡
            ax.imshow(img.numpy())
        else:
            # PILå›¾ç‰‡
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes

def get_dataloader_workers(num_process:int)->int:
    """ä½¿ç”¨4ä¸ªè¿›ç¨‹æ¥è¯»å–æ•°æ®

    Defined in :numref:`sec_fashion_mnist`"""
    return num_process

def load_data_fashion_mnist(batch_size, resize=None):
    """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­
    resize:int|none : æ˜¯å¦å°†å›¾åƒè°ƒæ•´ä¸ºæŒ‡å®šå°ºå¯¸ï¼ˆå¦‚ resize=32 å°† 28Ã—28 å›¾åƒæ”¾å¤§åˆ° 32Ã—32ï¼‰
    num_process : é»˜è®¤ä¸º8
    Defined in :numref:`sec_fashion_mnist`"""
    #å°† PIL å›¾åƒæˆ– NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡
    #å…³é”®è¡Œä¸ºï¼šå°†åƒç´ å€¼ä» [0, 255]ï¼ˆuint8ï¼‰ç¼©æ”¾åˆ° [0.0, 1.0]ï¼ˆfloat32ï¼‰
    #æ”¹å˜ç»´åº¦é¡ºåºï¼š(H, W, C) â†’ (C, H, W)ï¼ˆé€šé“å‰ç½®ï¼‰
    #è¾“å‡ºå¼ é‡å½¢çŠ¶ï¼šå¯¹äº Fashion-MNISTï¼ˆç°åº¦å›¾ï¼‰ï¼Œå˜ä¸º (1, 28, 28)
    trans = [transforms.ToTensor()] 
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../Fashion-MNIST_data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../Fashion-MNIST_data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers(num_process=8)),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers(num_process=8)))

def accuracy(y_hat, y):
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡

    Defined in :numref:`sec_softmax_scratch`"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = torch.argmax(y_hat, axis=1)
    cmp = y_hat.to(y.dtype) == y
    return float(cmp.type(y.dtype).sum())

def evaluate_accuracy(net, data_iter):
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦

    Defined in :numref:`sec_softmax_scratch`"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        """Defined in :numref:`sec_softmax_scratch`"""
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰

    Defined in :numref:`sec_softmax_scratch`"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]

def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """ç”¨äºåé¢lambdaé—­åŒ…
    """
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


class Animator:
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(8, 4.5)):
        """Defined in :numref:`sec_softmax_scratch`"""

        # å¢é‡åœ°ç»˜åˆ¶å¤šæ¡çº¿
        if legend is None:
            legend = []
        use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        #å¦‚æœåªæœ‰ 1 ä¸ªå­å›¾ï¼Œaxes æ˜¯å•ä¸ªå¯¹è±¡ï¼›ä½†ä¸ºäº†ç»Ÿä¸€å¤„ç†ï¼Œå¼ºåˆ¶è½¬ä¸ºåˆ—è¡¨ [axes]
        #è¿™æ ·åç»­ä»£ç æ€»èƒ½ç”¨ self.axes[0] è®¿é—®ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰åæ ‡è½´
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # ä½¿ç”¨lambdaå‡½æ•°æ•è·å‚æ•°
        self.config_axes = lambda: set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # å‘å›¾è¡¨ä¸­æ·»åŠ å¤šä¸ªæ•°æ®ç‚¹
        # æ”¯æŒæ ‡é‡æˆ–åˆ—è¡¨
        #å¦‚æœ y æ˜¯å•ä¸ªæ•°å­—ï¼ˆå¦‚ loss=0.5ï¼‰ï¼Œè½¬ä¸º [0.5]
        #å¦‚æœ x æ˜¯å•ä¸ªæ•°å­—ï¼ˆå¦‚ epoch=3ï¼‰ï¼Œå¤åˆ¶æˆ [3, 3, ..., 3]ï¼ˆé•¿åº¦ = y çš„æ¡æ•°ï¼‰
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        #éå†æ¯æ¡çº¿ï¼Œå°† (x, y) è¿½åŠ åˆ°å†å²è®°å½•ä¸­
        #è·³è¿‡ None å€¼ï¼ˆå¯ç”¨äºè·³è¿‡æŸäº› epoch ä¸ç”»ç‚¹ï¼‰
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla() # clear current axes
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        #æ¸…é™¤ä¸Šä¸€æ¬¡è¾“å‡º
        #wait=True è¡¨ç¤ºâ€œç­‰æ–°å†…å®¹å‡†å¤‡å¥½å†æ¸…é™¤â€ï¼Œé¿å…é—ªçƒ
        display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰

    Defined in :numref:`sec_softmax_scratch`"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 1],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
    print(f'train_loss ={train_metrics[0]},\n train_accuracy ={train_metrics[1]},\n test_accuracy={test_acc}')

# from .utils import argmax, reshape
def predict_ch3(net, test_iter, n=6):
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰
    é¢„æµ‹æ ‡ç­¾å¹¶å¯è§†åŒ–å‰ n ä¸ªæ ·æœ¬ï¼ˆFashion-MNIST ä¸“ç”¨ï¼‰
    
    å‚æ•°:
        net: æ¨¡å‹å‡½æ•°ï¼Œæ¥å— (batch_size, 784) è¾“å…¥ï¼Œè¾“å‡º (batch_size, 10) logits æˆ–æ¦‚ç‡
        test_iter: æµ‹è¯•æ•°æ®è¿­ä»£å™¨ï¼ˆDataLoaderï¼‰ï¼Œæ¯ä¸ª batch ä¸º (X, y)
        n: æ˜¾ç¤ºå‰ n å¼ å›¾åƒï¼ˆé»˜è®¤ 6ï¼‰
    Defined in :numref:`sec_softmax_scratch`"""
    for X, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(torch.argmax(net(X), axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        X[0:n].reshape(n, 28, 28), 1, n, titles=titles[0:n])


def evaluate_loss(net, data_iter, loss):
    """è¯„ä¼°ç»™å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„æŸå¤±

    Defined in :numref:`sec_model_selection`"""
    metric = Accumulator(2) # æŸå¤±çš„æ€»å’Œ,æ ·æœ¬æ•°é‡
    for X, y in data_iter:
        out = net(X)
        y = y.reshape(out.shape)
        l = loss(out, y)
        metric.add(torch.sum(l), l.numel())
    return metric[0] / metric[1]

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('..', 'data')):
    """ä¸‹è½½ä¸€ä¸ªDATA_HUBä¸­çš„æ–‡ä»¶ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶å

    Defined in :numref:`sec_kaggle_house`"""
    assert name in DATA_HUB, f"{name} ä¸å­˜åœ¨äº {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname  # å‘½ä¸­ç¼“å­˜
    print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname

def download_extract(name, folder=None):
    """ä¸‹è½½å¹¶è§£å‹zip/taræ–‡ä»¶

    Defined in :numref:`sec_kaggle_house`"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, 'åªæœ‰zip/taræ–‡ä»¶å¯ä»¥è¢«è§£å‹ç¼©'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir

def download_all():
    """ä¸‹è½½DATA_HUBä¸­çš„æ‰€æœ‰æ–‡ä»¶

    Defined in :numref:`sec_kaggle_house`"""
    for name in DATA_HUB:
        download(name)

DATA_HUB['kaggle_house_train'] = (
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

def try_gpu(i=0):
    """å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›gpu(i)ï¼Œå¦åˆ™è¿”å›cpu()

    Defined in :numref:`sec_use_gpu`"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():
    """è¿”å›æ‰€æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰GPUï¼Œåˆ™è¿”å›[cpu(),]

    Defined in :numref:`sec_use_gpu`"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
def corr2d(X, K):
    """
    è®¡ç®—äºŒç»´äº’ç›¸å…³ï¼ˆcross-correlationï¼‰

    å‚æ•°:
        X: è¾“å…¥å¼ é‡ï¼Œshape = (H, W)
        K: å·ç§¯æ ¸ï¼Œshape = (h, w)

    è¿”å›:
        Y: è¾“å‡ºå¼ é‡ï¼Œshape = (H-h+1, W-w+1)

    Defined in :numref:`sec_conv_layer`"""
    h, w = K.shape
    # 1. åˆ›å»ºè¾“å‡ºå¼ é‡ï¼ˆå…¨ 0ï¼‰
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1),
                    dtype=X.dtype,
                    device=X.device
                    )
    # 2. æ»‘åŠ¨çª—å£è®¡ç®—
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = torch.sum((X[i: i + h, j: j + w] * K))
    return Y

def evaluate_accuracy_gpu(net, data_iter, device=None):
    """ä½¿ç”¨GPUè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„ç²¾åº¦

    Defined in :numref:`sec_lenet`"""
    if isinstance(net, nn.Module):
        net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        if not device:
            device = next(iter(net.parameters())).device
    # æ­£ç¡®é¢„æµ‹çš„æ•°é‡ï¼Œæ€»é¢„æµ‹çš„æ•°é‡
    metric = Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERTå¾®è°ƒæ‰€éœ€çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

class Timer:
    """Record multiple running times."""
    def __init__(self):
        """Defined in :numref:`sec_minibatch_sgd`"""
        self.times = []
        self.start()

    def start(self):
        """Start the timer."""
        self.tik = time.time()

    def stop(self):
        """Stop the timer and record the time in a list."""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """Return the average time."""
        return sum(self.times) / len(self.times)

    def sum(self):
        """Return the sum of time."""
        return sum(self.times)

    def cumsum(self):
        """Return the accumulated time."""
        return np.array(self.times).cumsum().tolist()

def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)

    Defined in :numref:`sec_lenet`"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = Animator(
        xlabel='epoch', 
        xlim=[1, num_epochs],
        legend=['train loss', 'train acc', 'test acc']
        )
    timer, num_batches = Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')

class Residual(nn.Module):
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)

class Benchmark:
    """ç”¨äºæµ‹é‡è¿è¡Œæ—¶é—´"""
    def __init__(self, description='Done'):
        """Defined in :numref:`sec_hybridize`"""
        self.description = description

    def __enter__(self):
        self.timer = Timer()
        return self

    def __exit__(self, *args):
        print(f'{self.description}: {self.timer.stop():.4f} sec')

DATA_HUB['time_machine'] = (
    DATA_URL + 'timemachine.txt',
    '090b5e7e70c295757f55df93cb0a180b9691891a'
    )

def read_time_machine():
    """å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œçš„åˆ—è¡¨ä¸­

    Defined in :numref:`sec_text_preprocessing`"""
    with open(download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def tokenize(lines, token='word'):
    """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ

    Defined in :numref:`sec_text_preprocessing`"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

class Vocab:
    """æ–‡æœ¬è¯è¡¨"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        """
        tokens: äºŒç»´æˆ–ä¸€ç»´ token åˆ—è¡¨ï¼ˆå¦‚ [['a','b'], ['c']]ï¼‰
        min_freq: æœ€å°å‡ºç°é¢‘æ¬¡ï¼Œä½äºæ­¤å€¼çš„ token è¢«å¿½ç•¥
        reserved_tokens: é¢„ç•™ tokenï¼ˆå¦‚ ['<pad>', '<bos>']ï¼‰ï¼Œæ€»æ˜¯åŒ…å«åœ¨è¯è¡¨å¼€å¤´

        å†…éƒ¨ç»“æ„ï¼š
        idx_to_token: åˆ—è¡¨ï¼Œç´¢å¼• â†’ tokenï¼ˆå¦‚ [0:'<unk>', 1:'a', 2:'b', ...]ï¼‰
        token_to_idx: å­—å…¸ï¼Œtoken â†’ ç´¢å¼•ï¼ˆå¦‚ {'a':1, 'b':2, ...}ï¼‰
        _token_freqs: æŒ‰é¢‘æ¬¡é™åºæ’åˆ—çš„ (token, freq) åˆ—è¡¨
        
        ç‰¹æ®Šè®¾è®¡ï¼š
        <unk> å›ºå®šç´¢å¼•ä¸º 0ï¼šä»»ä½•æœªç™»å½•è¯ï¼ˆOOVï¼‰éƒ½æ˜ å°„åˆ° 0
        é¢„ç•™ token ä¼˜å…ˆæ’å…¥ï¼šç¡®ä¿ <pad> ç­‰æœ‰å›ºå®š ID
        
        Defined in :numref:`sec_text_preprocessing`"""
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        """åŠŸèƒ½ï¼štoken â†’ index çš„ä¾¿æ·æ¥å£
        æ”¯æŒä¸¤ç§è°ƒç”¨ï¼š
        vocab['hello'] â†’ è¿”å› IDï¼ˆè‹¥ä¸å­˜åœ¨ï¼Œè¿”å› 0ï¼‰
        vocab[['h','e','l']] â†’ è¿”å› [ID_h, ID_e, ID_l]
        """
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        """
        åŠŸèƒ½ï¼šindex â†’ token çš„åå‘æ˜ å°„
        ç”¨é€”ï¼šå°†æ¨¡å‹è¾“å‡ºçš„ ID åºåˆ—è½¬å›å¯è¯»æ–‡æœ¬
        """
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):
    """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡

    Defined in :numref:`sec_text_preprocessing`"""
    # è¿™é‡Œçš„tokensæ˜¯1Dåˆ—è¡¨æˆ–2Dåˆ—è¡¨
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä¸€ä¸ªåˆ—è¡¨
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

def load_corpus_time_machine(max_tokens=-1):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨

    Defined in :numref:`sec_text_preprocessing`"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

def seq_data_iter_random(corpus, batch_size, num_steps):
    """ä½¿ç”¨éšæœºæŠ½æ ·ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—

    ä»ä¸€ä¸ªé•¿åºåˆ— corpusï¼ˆå¦‚ [1,2,3,4,5,6,7,8,...]ï¼‰ä¸­ï¼ŒéšæœºæŠ½å–ä¸é‡å çš„å­åºåˆ—ï¼Œæ¯ä¸ªå­åºåˆ—é•¿åº¦ä¸º num_stepsï¼Œå¹¶æ„é€ ï¼š
    è¾“å…¥ Xï¼šå­åºåˆ—çš„å‰ num_steps ä¸ª tokenï¼ˆå†å²ä¸Šä¸‹æ–‡ï¼‰
    æ ‡ç­¾ Yï¼šå­åºåˆ—çš„å num_steps ä¸ª tokenï¼ˆå³ X å‘å³å¹³ç§» 1 ä½ï¼‰
    
    Defined in :numref:`sec_language_model`"""

    # ä»éšæœºåç§»é‡å¼€å§‹å¯¹åºåˆ—è¿›è¡Œåˆ†åŒºï¼ŒéšæœºèŒƒå›´åŒ…æ‹¬num_steps-1
    # é¿å…æ‰€æœ‰ batch æ€»æ˜¯ä»åºåˆ—çš„â€œæ•´æ•°å€ä½ç½®â€å¼€å§‹ï¼ˆå¦‚ 0, L, 2L...ï¼‰ï¼Œå¢åŠ éšæœºæ€§ã€‚
    corpus = corpus[random.randint(0, num_steps - 1):]
    # å‡å»1ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬éœ€è¦è€ƒè™‘æ ‡ç­¾
    # æ¯ä¸ªå­åºåˆ—éœ€è¦ num_steps ä¸ªè¾“å…¥ + num_steps ä¸ªæ ‡ç­¾ï¼Œä½†æ ‡ç­¾æ˜¯è¾“å…¥çš„â€œä¸‹ä¸€ä¸ªâ€
    # æ€»å…±éœ€è¦ num_steps + 1 ä¸ªè¿ç»­ tokenï¼
    num_subseqs = (len(corpus) - 1) // num_steps
    # é•¿åº¦ä¸ºnum_stepsçš„å­åºåˆ—çš„èµ·å§‹ç´¢å¼•
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # åœ¨éšæœºæŠ½æ ·çš„è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œ
    # æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„ã€éšæœºçš„ã€å°æ‰¹é‡ä¸­çš„å­åºåˆ—ä¸ä¸€å®šåœ¨åŸå§‹åºåˆ—ä¸Šç›¸é‚»
    random.shuffle(initial_indices)

    def data(pos):
        # è¿”å›ä»posä½ç½®å¼€å§‹çš„é•¿åº¦ä¸ºnum_stepsçš„åºåˆ—
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # åœ¨è¿™é‡Œï¼Œinitial_indicesåŒ…å«å­åºåˆ—çš„éšæœºèµ·å§‹ç´¢å¼•
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        # å°†åˆ—è¡¨è½¬ä¸º PyTorch å¼ é‡
        # X å’Œ Y çš„å½¢çŠ¶å‡ä¸º (batch_size, num_steps)
        # ä½¿ç”¨ yield å®ç°å†…å­˜é«˜æ•ˆçš„è¿­ä»£å™¨
        yield torch.tensor(X), torch.tensor(Y)

def seq_data_iter_sequential(corpus, batch_size, num_steps):
    """ä½¿ç”¨é¡ºåºåˆ†åŒºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—
    å®ƒç‰¹åˆ«é€‚ç”¨äºéœ€è¦ç»´æŒåºåˆ—è¿ç»­æ€§çš„åœºæ™¯ï¼Œæ¯”å¦‚ RNN è®­ç»ƒä¸­å¸Œæœ›è·¨ batch ä¼ é€’éšè—çŠ¶æ€ã€‚

    ä»é•¿åºåˆ— corpus ä¸­ï¼ŒæŒ‰é¡ºåºã€ä¸æ‰“ä¹±åœ°åˆ’åˆ†å‡ºå¤šä¸ª batchï¼Œæ¯ä¸ª batch åŒ…å« batch_size æ¡é•¿åº¦ä¸º num_steps çš„å­åºåˆ—ï¼Œå¹¶ä¿è¯ï¼š
    - åŒä¸€æ‰¹å†…ï¼šä¸åŒæ ·æœ¬æ˜¯åŸå§‹åºåˆ—ä¸­ç­‰é—´éš”çš„ç‰‡æ®µï¼ˆç”¨äºå¹¶è¡Œï¼‰
    - æ‰¹ä¸æ‰¹ä¹‹é—´ï¼šåœ¨åŸå§‹åºåˆ—ä¸­è¿ç»­è¡”æ¥ï¼ˆå¯ç”¨äºçŠ¶æ€ä¼ é€’ï¼‰
    âœ… è¿™æ˜¯ stateful RNN è®­ç»ƒçš„æ ‡å‡†æ–¹å¼ã€‚
    """
    # ä»éšæœºåç§»é‡å¼€å§‹åˆ’åˆ†åºåˆ—
    # éšæœºèµ·å§‹åç§»ï¼ˆé¿å…å›ºå®šå¯¹é½ï¼‰
    offset = random.randint(0, num_steps)
    # æˆ‘ä»¬éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š
    # - X å’Œ Y éƒ½è¦æœ‰è¶³å¤Ÿé•¿åº¦ â†’ éœ€è¦ num_tokens + 1 ä¸ªåŸå§‹ tokenï¼ˆY æ¯” X å¤šä¸€ä¸ªä½ç½®ï¼‰
    # - æ€» token æ•°å¿…é¡»èƒ½è¢« batch_size æ•´é™¤ â†’ ä»¥ä¾¿ reshape æˆ (batch_size, T)
    # æ‰€ä»¥ï¼š
    # å¯ç”¨é•¿åº¦ = len(corpus) - offsetï¼ˆå»æ‰å¼€å¤´åç§»ï¼‰
    # ä½† Y éœ€è¦å†å¾€åä¸€ä¸ªä½ç½® â†’ æœ€å¤§å¯ç”¨ = len(corpus) - offset - 1
    # ç„¶åå‘ä¸‹å–æ•´åˆ° batch_size çš„å€æ•°ï¼š// batch_size * batch_size
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size

    # æ„é€ å®Œæ•´çš„ Xs å’Œ Ysï¼ˆå…¨å±€è§†å›¾ï¼‰
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])

    # é‡å¡‘ä¸º (batch_size, T) â€”â€” â€œå‚ç›´åˆ‡åˆ†â€ 
    # å°†é•¿åºåˆ—â€œç«–ç€â€åˆ‡æˆ batch_size æ¡å¹³è¡Œè½¨é“
    """
    Xs = [
    [x1, x4, x7, x10],   # è½¨é“ 0
    [x2, x5, x8, x11],   # è½¨é“ 1
    [x3, x6, x9, x12]    # è½¨é“ 2
    ]
    âœ… æ¯æ¡è½¨é“æ˜¯åŸå§‹åºåˆ—ä¸­æ¯éš” batch_size ä¸ªä½ç½®å–ä¸€ä¸ª token
    âœ… è½¨é“ä¹‹é—´åœ¨æ—¶é—´ä¸Šæ˜¯äº¤é”™çš„ï¼Œä½†å„è‡ªä¿æŒè¿ç»­

    ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ
    - å…è®¸åœ¨ä¸€ä¸ª batch ä¸­å¹¶è¡Œå¤„ç† batch_size ä¸ªç‹¬ç«‹ä½†è¿ç»­çš„å­åºåˆ—
    - å¯¹äº RNNï¼Œæ¯æ¡è½¨é“å¯ä»¥ç»´æŠ¤è‡ªå·±çš„éšè—çŠ¶æ€
    """
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    # é€ batch ç”Ÿæˆï¼ˆæ»‘åŠ¨çª—å£ï¼‰ 
    # æ²¿æ—¶é—´ç»´åº¦æ»‘åŠ¨çª—å£ï¼Œæ¯æ¬¡å– num_steps åˆ—
    # X å’Œ Y å½¢çŠ¶å‡ä¸º (batch_size, num_steps)
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y

class SeqDataLoader:
    """åŠ è½½åºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        """Defined in :numref:`sec_language_model`"""
        if use_random_iter:
            self.data_iter_fn = seq_data_iter_random
        else:
            self.data_iter_fn = seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps,
                           use_random_iter=False, max_tokens=10000):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¿­ä»£å™¨å’Œè¯è¡¨

    Defined in :numref:`sec_language_model`"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab

class RNNModelScratch:
    """ä»é›¶å¼€å§‹å®ç°çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        """Defined in :numref:`sec_rnn_scratch`"""
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)

def predict_ch8(prefix, num_preds, net, vocab, device):
    """åœ¨prefixåé¢ç”Ÿæˆæ–°å­—ç¬¦

    Defined in :numref:`sec_rnn_scratch`"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: d2l.reshape(d2l.tensor(
        [outputs[-1]], device=device), (1, 1))
    for y in prefix[1:]:  # é¢„çƒ­æœŸ
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # é¢„æµ‹num_predsæ­¥
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])

def grad_clipping(net, theta):
    """è£å‰ªæ¢¯åº¦

    Defined in :numref:`sec_rnn_scratch`"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm

def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """è®­ç»ƒç½‘ç»œä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬8ç« ï¼‰

    Defined in :numref:`sec_rnn_scratch`"""
    state, timer = None, Timer()
    metric = Accumulator(2)  # è®­ç»ƒæŸå¤±ä¹‹å’Œ,è¯å…ƒæ•°é‡
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æˆ–ä½¿ç”¨éšæœºæŠ½æ ·æ—¶åˆå§‹åŒ–state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # stateå¯¹äºnn.GRUæ˜¯ä¸ªå¼ é‡
                state.detach_()
            else:
                # stateå¯¹äºnn.LSTMæˆ–å¯¹äºæˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°çš„æ¨¡å‹æ˜¯ä¸ªå¼ é‡
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # å› ä¸ºå·²ç»è°ƒç”¨äº†meanå‡½æ•°
            updater(batch_size=1)
        metric.add(l * d2l.size(y), d2l.size(y))
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()

def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬8ç« ï¼‰

    Defined in :numref:`sec_rnn_scratch`"""
    loss = nn.CrossEntropyLoss()
    animator = Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])
    # åˆå§‹åŒ–
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)
    # è®­ç»ƒå’Œé¢„æµ‹
    for epoch in range(num_epochs):
        ppl, speed = train_epoch_ch8(
            net, train_iter, loss, updater, device, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    print(f'å›°æƒ‘åº¦ {ppl:.1f}, {speed:.1f} è¯å…ƒ/ç§’ {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))

class RNNModel(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹

    Defined in :numref:`sec_rnn-concise`"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # å¦‚æœRNNæ˜¯åŒå‘çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰ï¼Œnum_directionsåº”è¯¥æ˜¯2ï¼Œå¦åˆ™åº”è¯¥æ˜¯1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # å…¨è¿æ¥å±‚é¦–å…ˆå°†Yçš„å½¢çŠ¶æ”¹ä¸º(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)
        # å®ƒçš„è¾“å‡ºå½¢çŠ¶æ˜¯(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,è¯è¡¨å¤§å°)ã€‚
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRUä»¥å¼ é‡ä½œä¸ºéšçŠ¶æ€
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # nn.LSTMä»¥å…ƒç»„ä½œä¸ºéšçŠ¶æ€
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))

#d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
#                          '94646ad1522d915e7b0f9296181140edcf86a4f5')