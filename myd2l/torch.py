# /home/d2l/d2l/myd2l/torch.py
#æœ¬åŒ…ç”¨äºd2lå­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¸ªäººè°ƒè¯•ä¸æ›´æ–°ï¼Œç”¨äºé‡æ„d2l/torch.py
#è¿˜åŒ…å«/home/d2l/d2l/setup.py &  
# /home/d2l/d2l/pyproject.toml & 
# /home/d2l/d2l/myd2l.egg-info ç”¨äºæ„å»ºæœ¬åœ°pkg

#ä»£ç æ›´æ–°å,åœ¨ipykernelè¿è¡Œæ—¶éœ€è¦é‡å¯kernel

import numpy as np
import torch
import torchvision
from PIL import Image
from torch import nn
from torch.nn import functional as F
from torch.utils import data
from torchvision import transforms

nn_Module = nn.Module

#################   WARNING   ################
# The below part is generated automatically through:
#    d2lbook build lib
# Don't edit it directly

import collections
import hashlib
import math
import os
import random
import re
import shutil
import sys
import tarfile
import time
import zipfile
from collections import defaultdict
import pandas as pd
import requests
from IPython import display
from matplotlib import pyplot as plt
from matplotlib_inline import backend_inline

def use_svg_display():
    """ä½¿ç”¨svgæ ¼å¼åœ¨Jupyterä¸­æ˜¾ç¤ºç»˜å›¾
    è®© Jupyter Notebook ä¸­çš„ Matplotlib å›¾å½¢ä»¥ SVGï¼ˆçŸ¢é‡å›¾ï¼‰æ ¼å¼ æ˜¾ç¤ºï¼Œè€Œéé»˜è®¤çš„ PNGã€‚
    backend_inline æ˜¯ matplotlib-inline åŒ…æä¾›çš„æ¨¡å—ï¼ˆJupyter å†…è”åç«¯ï¼‰ã€‚
    æ­¤è°ƒç”¨å‘Šè¯‰ Jupyterï¼šâ€œä»¥åæ‰€æœ‰ matplotlib å›¾éƒ½ç”¨ SVG æ ¼å¼æ¸²æŸ“â€ã€‚
    ğŸ’¡ æ³¨æ„ï¼šæ­¤è®¾ç½®å¯¹å½“å‰ notebook ä¼šè¯å…¨å±€ç”Ÿæ•ˆã€‚

    Defined in :numref:`sec_calculus`"""
    backend_inline.set_matplotlib_formats('svg') 

def set_figsize(figsize=(3.5, 2.5)):
    """è®¾ç½®matplotlibçš„å›¾è¡¨å¤§å°
    plt.rcParams æ˜¯ Matplotlib çš„å…¨å±€é…ç½®å­—å…¸ã€‚
    'figure.figsize' æ§åˆ¶å›¾å½¢é»˜è®¤å®½é«˜ï¼ˆå•ä½ï¼šè‹±å¯¸ï¼‰ã€‚

    Defined in :numref:`sec_calculus`"""
    use_svg_display()
    plt.rcParams['figure.figsize'] = figsize

def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """è®¾ç½®matplotlibçš„è½´
    
    Defined in :numref:`sec_calculus`"""
    axes.set_xlabel(xlabel) # è®¾ç½® x è½´æ ‡ç­¾ã€‚
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale) # è®¾ç½® x è½´åˆ»åº¦ç±»å‹ï¼ˆçº¿æ€§/å¯¹æ•°ç­‰ï¼‰
    axes.set_yscale(yscale)
    axes.set_xlim(xlim) # è®¾ç½® x è½´æ˜¾ç¤ºèŒƒå›´ï¼ˆå¿…é¡»æ˜¯äºŒå…ƒ tuple/listï¼‰ã€‚
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend) # å¦‚æœ legend éç©ºï¼ˆä¸æ˜¯ None æˆ–ç©ºåˆ—è¡¨ï¼‰ï¼Œåˆ™æ˜¾ç¤ºå›¾ä¾‹ã€‚
    axes.grid() #æ˜¾ç¤ºç½‘æ ¼çº¿ï¼Œä¾¿äºè¯»å›¾ã€‚ 


def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """ç»˜åˆ¶æ•°æ®ç‚¹
    x åæ ‡æ•°æ®ï¼ˆå¯ä¸º listã€tensorã€ndarrayï¼‰
    y åæ ‡æ•°æ®ï¼›è‹¥ä¸º Noneï¼Œåˆ™ X è¢«å½“ä½œ y å€¼ï¼Œx è‡ªåŠ¨ä¸ºç´¢å¼•
    fmts: æ¯æ¡çº¿çš„æ ·å¼ï¼ˆé¢œè‰²+çº¿å‹ï¼‰,é»˜è®¤å€¼ ('-', 'm--', ...)

    Defined in :numref:`sec_calculus`"""
    if legend is None:
        legend = []

    set_figsize(figsize)
    axes = axes if axes else plt.gca()

    # å¦‚æœXæœ‰ä¸€ä¸ªè½´ï¼Œè¾“å‡ºTrueï¼Œ åˆ¤æ–­ X æ˜¯å¦æ˜¯â€œä¸€ç»´æ•°æ®â€ã€‚
    def has_one_axis(X):
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y): # å¦‚æœ X åªæœ‰ä¸€ç»„ï¼Œä½† Y æœ‰å¤šç»„ï¼ˆå¸¸è§æƒ…å†µï¼‰ï¼Œåˆ™å¤åˆ¶ X ä½¿å…¶åŒ¹é… Y çš„æ•°é‡ã€‚
        X = X * len(Y)
    axes.cla() # æ¸…ç©ºå½“å‰åæ ‡è½´ï¼Œé¿å…æ—§å›¾æ®‹ç•™ã€‚
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


class Timer:
    """è®°å½•å¤šæ¬¡è¿è¡Œæ—¶é—´"""
    def __init__(self):
        """Defined in :numref:`subsec_linear_model`"""
        self.times = []
        self.start()

    def start(self):
        """å¯åŠ¨è®¡æ—¶å™¨"""
        self.tik = time.time()

    def stop(self):
        """åœæ­¢è®¡æ—¶å™¨å¹¶å°†æ—¶é—´è®°å½•åœ¨åˆ—è¡¨ä¸­"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """è¿”å›å¹³å‡æ—¶é—´"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """è¿”å›æ—¶é—´æ€»å’Œ"""
        return sum(self.times)

    def cumsum(self):
        """è¿”å›ç´¯è®¡æ—¶é—´"""
        return np.array(self.times).cumsum().tolist()

def synthetic_data(w, b, num_examples):
    """ç”Ÿæˆy=Xw+b+å™ªå£° é€ ä¸€ä¸ªâ€œå¯æ§çš„æ•°æ®é›†â€
    w: çœŸå®æƒé‡ï¼ˆtensorï¼Œå½¢çŠ¶ (d,)ï¼‰
    b: çœŸå®åç½®ï¼ˆæ ‡é‡æˆ– 0-d tensorï¼‰
    num_examples: æ ·æœ¬æ•°

    Defined in :numref:`sec_linear_scratch`"""
    X = torch.normal(0, 1, (num_examples, len(w))) # æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ª len(w) ç»´å‘é‡ï¼Œç‰¹å¾æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1)) 

def linreg(X, w, b):
    """çº¿æ€§å›å½’æ¨¡å‹

    Defined in :numref:`sec_linear_scratch`"""
    return torch.matmul(X, w) + b

def squared_loss(y_hat, y):
    """å‡æ–¹æŸå¤±
    reshapeï¼š ç¡®ä¿y å’Œ y_hat å½¢çŠ¶ä¸€è‡´ï¼Œé¿å…å¹¿æ’­é”™è¯¯
    Defined in :numref:`sec_linear_scratch`"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  

def sgd(params, lr, batch_size):
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
    params: [w, b]ï¼ˆtensor åˆ—è¡¨ï¼‰
    lr: å­¦ä¹ ç‡
    batch_size: æ‰¹å¤§å°

    Defined in :numref:`sec_linear_scratch`"""
    with torch.no_grad(): # å‘Šè¯‰ autogradï¼šä¸‹é¢çš„æ“ä½œä¸è¦å»ºè®¡ç®—å›¾
        for param in params:
            param -= lr * param.grad / batch_size # ç´¯ç§¯çš„æ¢¯åº¦/ æ‰¹å¤§å° = å¹³å‡æ¢¯åº¦
            param.grad.zero_() #  å¿…é¡»æœ‰ï¼PyTorch é»˜è®¤æ¢¯åº¦æ˜¯ ç´¯ç§¯çš„

def load_array(data_arrays, batch_size, is_train=True):
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨
    torch.utils.data.TensorDataset ï¼šæ˜¯ä¸€ä¸ªæ•°æ®é›†åŒ…è£…å™¨ï¼Œç”¨äºå°†å¤šä¸ªå¼ é‡æ‰“åŒ…æˆä¸€ä¸ªæ•°æ®é›†ï¼Œè¦æ±‚æ‰€æœ‰å¼ é‡çš„ç¬¬ä¸€ç»´ï¼ˆæ ·æœ¬æ•°ï¼‰ç›¸åŒã€‚
                                    æ¯æ¬¡ç´¢å¼• dataset[i] è¿”å› (tensor1[i], tensor2[i], ..., tensorN[i])
                                    å¸¸ç”¨äºï¼š(X, y) é…å¯¹ï¼Œå³ç‰¹å¾å’Œæ ‡ç­¾ä¸€èµ·è¿­ä»£
    e.g.: 
    X = torch.randn(100, 2)   # 100 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾
    y = torch.randn(100, 1)   # 100 ä¸ªæ ‡ç­¾

    dataset = TensorDataset(X, y)
    print(dataset[0])  # è¾“å‡º: (tensor([x1, x2]), tensor([y1]))

    * æ˜¯ Python çš„ â€œè§£åŒ…æ“ä½œç¬¦â€ï¼ˆunpacking operatorï¼‰ å®ƒæŠŠä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å…ƒç»„ï¼‰å±•å¼€ä¸ºå¤šä¸ªç‹¬ç«‹å‚æ•°ã€‚

    data_arrays = (features, labels)  # ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå¼ é‡çš„å…ƒç»„
    # ä¸ä½¿ç”¨ *ï¼š
    TensorDataset(data_arrays)        # âŒ é”™è¯¯ï¼ä¼ å…¥çš„æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œä¸æ˜¯ä¸¤ä¸ªå¼ é‡

    # ä½¿ç”¨ *ï¼š
    TensorDataset(*data_arrays)       # âœ… ç­‰ä»·äº TensorDataset(features, labels)
    
    Defined in :numref:`sec_linear_concise`"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

def get_fashion_mnist_labels(labels):
    """è¿”å›Fashion-MNISTæ•°æ®é›†çš„æ–‡æœ¬æ ‡ç­¾

    Defined in :numref:`sec_fashion_mnist`"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    """ç»˜åˆ¶å›¾åƒåˆ—è¡¨

    Defined in :numref:`sec_fashion_mnist`"""
    figsize = (num_cols * scale, num_rows * scale)
    #å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ fig å¯¹è±¡ï¼ˆä¸è¿›è¡Œä¿å­˜ã€è°ƒæ•´æ•´ä½“å¸ƒå±€ç­‰æ“ä½œï¼‰
    #ç”¨ _ æ˜¯ Python æƒ¯ä¾‹ï¼Œè¡¨ç¤ºâ€œå¿½ç•¥è¿™ä¸ªå˜é‡â€
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    # å°† 2D å­å›¾æ•°ç»„è½¬ä¸º 1Dï¼Œä¾¿äºé¡ºåºéå†
    axes = axes.flatten()
    #zip(axes, imgs) å°† axesï¼ˆå­å›¾åˆ—è¡¨ï¼‰å’Œ imgsï¼ˆå›¾åƒåˆ—è¡¨ï¼‰é…å¯¹,ç”Ÿæˆè¿­ä»£å™¨ï¼š(axes[0], imgs[0]), (axes[1], imgs[1])
    #enumerate(...) ç»™æ¯ä¸ªé…å¯¹åŠ ä¸Šç´¢å¼• iï¼š(0, (ax0, img0)), (1, (ax1, img1)), ...
    for i, (ax, img) in enumerate(zip(axes, imgs)): 
        if torch.is_tensor(img):
            # å›¾ç‰‡å¼ é‡
            ax.imshow(img.numpy())
        else:
            # PILå›¾ç‰‡
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes

def get_dataloader_workers(num_process:int)->int:
    """ä½¿ç”¨4ä¸ªè¿›ç¨‹æ¥è¯»å–æ•°æ®

    Defined in :numref:`sec_fashion_mnist`"""
    return num_process

def load_data_fashion_mnist(batch_size, resize=None):
    """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­
    resize:int|none : æ˜¯å¦å°†å›¾åƒè°ƒæ•´ä¸ºæŒ‡å®šå°ºå¯¸ï¼ˆå¦‚ resize=32 å°† 28Ã—28 å›¾åƒæ”¾å¤§åˆ° 32Ã—32ï¼‰
    num_process : é»˜è®¤ä¸º8
    Defined in :numref:`sec_fashion_mnist`"""
    #å°† PIL å›¾åƒæˆ– NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡
    #å…³é”®è¡Œä¸ºï¼šå°†åƒç´ å€¼ä» [0, 255]ï¼ˆuint8ï¼‰ç¼©æ”¾åˆ° [0.0, 1.0]ï¼ˆfloat32ï¼‰
    #æ”¹å˜ç»´åº¦é¡ºåºï¼š(H, W, C) â†’ (C, H, W)ï¼ˆé€šé“å‰ç½®ï¼‰
    #è¾“å‡ºå¼ é‡å½¢çŠ¶ï¼šå¯¹äº Fashion-MNISTï¼ˆç°åº¦å›¾ï¼‰ï¼Œå˜ä¸º (1, 28, 28)
    trans = [transforms.ToTensor()] 
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../Fashion-MNIST_data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../Fashion-MNIST_data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers(num_process=8)),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers(num_process=8)))

def accuracy(y_hat, y):
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡

    Defined in :numref:`sec_softmax_scratch`"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = torch.argmax(y_hat, axis=1)
    cmp = y_hat.to(y.dtype) == y
    return float(cmp.type(y.dtype).sum())

def evaluate_accuracy(net, data_iter):
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦

    Defined in :numref:`sec_softmax_scratch`"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        """Defined in :numref:`sec_softmax_scratch`"""
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰

    Defined in :numref:`sec_softmax_scratch`"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]

def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """ç”¨äºåé¢lambdaé—­åŒ…
    """
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


class Animator:
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(16, 9)):
        """Defined in :numref:`sec_softmax_scratch`"""

        # å¢é‡åœ°ç»˜åˆ¶å¤šæ¡çº¿
        if legend is None:
            legend = []
        use_svg_display()
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        #å¦‚æœåªæœ‰ 1 ä¸ªå­å›¾ï¼Œaxes æ˜¯å•ä¸ªå¯¹è±¡ï¼›ä½†ä¸ºäº†ç»Ÿä¸€å¤„ç†ï¼Œå¼ºåˆ¶è½¬ä¸ºåˆ—è¡¨ [axes]
        #è¿™æ ·åç»­ä»£ç æ€»èƒ½ç”¨ self.axes[0] è®¿é—®ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰åæ ‡è½´
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # ä½¿ç”¨lambdaå‡½æ•°æ•è·å‚æ•°
        self.config_axes = lambda: set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # å‘å›¾è¡¨ä¸­æ·»åŠ å¤šä¸ªæ•°æ®ç‚¹
        # æ”¯æŒæ ‡é‡æˆ–åˆ—è¡¨
        #å¦‚æœ y æ˜¯å•ä¸ªæ•°å­—ï¼ˆå¦‚ loss=0.5ï¼‰ï¼Œè½¬ä¸º [0.5]
        #å¦‚æœ x æ˜¯å•ä¸ªæ•°å­—ï¼ˆå¦‚ epoch=3ï¼‰ï¼Œå¤åˆ¶æˆ [3, 3, ..., 3]ï¼ˆé•¿åº¦ = y çš„æ¡æ•°ï¼‰
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        #éå†æ¯æ¡çº¿ï¼Œå°† (x, y) è¿½åŠ åˆ°å†å²è®°å½•ä¸­
        #è·³è¿‡ None å€¼ï¼ˆå¯ç”¨äºè·³è¿‡æŸäº› epoch ä¸ç”»ç‚¹ï¼‰
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla() # clear current axes
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        #æ¸…é™¤ä¸Šä¸€æ¬¡è¾“å‡º
        #wait=True è¡¨ç¤ºâ€œç­‰æ–°å†…å®¹å‡†å¤‡å¥½å†æ¸…é™¤â€ï¼Œé¿å…é—ªçƒ
        display.clear_output(wait=True)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰

    Defined in :numref:`sec_softmax_scratch`"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc


from .utils import argmax, reshape
def predict_ch3(net, test_iter, n=6):
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰
    é¢„æµ‹æ ‡ç­¾å¹¶å¯è§†åŒ–å‰ n ä¸ªæ ·æœ¬ï¼ˆFashion-MNIST ä¸“ç”¨ï¼‰
    
    å‚æ•°:
        net: æ¨¡å‹å‡½æ•°ï¼Œæ¥å— (batch_size, 784) è¾“å…¥ï¼Œè¾“å‡º (batch_size, 10) logits æˆ–æ¦‚ç‡
        test_iter: æµ‹è¯•æ•°æ®è¿­ä»£å™¨ï¼ˆDataLoaderï¼‰ï¼Œæ¯ä¸ª batch ä¸º (X, y)
        n: æ˜¾ç¤ºå‰ n å¼ å›¾åƒï¼ˆé»˜è®¤ 6ï¼‰
    Defined in :numref:`sec_softmax_scratch`"""
    for X, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(torch.argmax(net(X), axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        X[0:n].reshape(n, 28, 28), 1, n, titles=titles[0:n])
