{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaad6d5",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    ":label:`sec_text_preprocessing`\n",
    "\n",
    "å¯¹äºåºåˆ—æ•°æ®å¤„ç†é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ :numref:`sec_sequence`ä¸­\n",
    "è¯„ä¼°äº†æ‰€éœ€çš„ç»Ÿè®¡å·¥å…·å’Œé¢„æµ‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚\n",
    "è¿™æ ·çš„æ•°æ®å­˜åœ¨è®¸å¤šç§å½¢å¼ï¼Œæ–‡æœ¬æ˜¯æœ€å¸¸è§ä¾‹å­ä¹‹ä¸€ã€‚\n",
    "ä¾‹å¦‚ï¼Œä¸€ç¯‡æ–‡ç« å¯ä»¥è¢«ç®€å•åœ°çœ‹ä½œä¸€ä¸²å•è¯åºåˆ—ï¼Œç”šè‡³æ˜¯ä¸€ä¸²å­—ç¬¦åºåˆ—ã€‚\n",
    "æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è§£ææ–‡æœ¬çš„å¸¸è§é¢„å¤„ç†æ­¥éª¤ã€‚\n",
    "è¿™äº›æ­¥éª¤é€šå¸¸åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. å°†æ–‡æœ¬ä½œä¸ºå­—ç¬¦ä¸²åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n",
    "1. å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºè¯å…ƒï¼ˆå¦‚å•è¯å’Œå­—ç¬¦ï¼‰ã€‚\n",
    "1. å»ºç«‹ä¸€ä¸ªè¯è¡¨ï¼Œå°†æ‹†åˆ†çš„è¯å…ƒæ˜ å°„åˆ°æ•°å­—ç´¢å¼•ã€‚\n",
    "1. å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•åºåˆ—ï¼Œæ–¹ä¾¿æ¨¡å‹æ“ä½œã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8907ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:24.243885Z",
     "iopub.status.busy": "2023-08-18T07:02:24.243343Z",
     "iopub.status.idle": "2023-08-18T07:02:26.213654Z",
     "shell.execute_reply": "2023-08-18T07:02:26.212745Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from myd2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987bf4c",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## è¯»å–æ•°æ®é›†\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬ä»H.G.Wellçš„[æ—¶å…‰æœºå™¨](https://www.gutenberg.org/ebooks/35)ä¸­åŠ è½½æ–‡æœ¬ã€‚\n",
    "è¿™æ˜¯ä¸€ä¸ªç›¸å½“å°çš„è¯­æ–™åº“ï¼Œåªæœ‰30000å¤šä¸ªå•è¯ï¼Œä½†è¶³å¤Ÿæˆ‘ä»¬å°è¯•ç‰›åˆ€ï¼Œ\n",
    "è€Œç°å®ä¸­çš„æ–‡æ¡£é›†åˆå¯èƒ½ä¼šåŒ…å«æ•°åäº¿ä¸ªå•è¯ã€‚\n",
    "ä¸‹é¢çš„å‡½æ•°(**å°†æ•°æ®é›†è¯»å–åˆ°ç”±å¤šæ¡æ–‡æœ¬è¡Œç»„æˆçš„åˆ—è¡¨ä¸­**)ï¼Œå…¶ä¸­æ¯æ¡æ–‡æœ¬è¡Œéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚\n",
    "ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¿½ç•¥äº†æ ‡ç‚¹ç¬¦å·å’Œå­—æ¯å¤§å†™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0f9f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.218338Z",
     "iopub.status.busy": "2023-08-18T07:02:26.217685Z",
     "iopub.status.idle": "2023-08-18T07:02:26.304928Z",
     "shell.execute_reply": "2023-08-18T07:02:26.304151Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# æ–‡æœ¬æ€»è¡Œæ•°: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œçš„åˆ—è¡¨ä¸­\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ï¼Œå°†æ‰€æœ‰çš„éæ–‡æœ¬ä¿¡æ¯è½¬æ¢ä¸ºç©ºæ ¼\n",
    "    # æœ‰æŸæ“ä½œï¼Œä½†æ˜¯ä¾¿äºåç»­è¯­æ–™åº“å¤„ç†\n",
    "    # .strip() æ–‡æœ¬ä¿¡æ¯çš„è§„æ•´å¤„ç†\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34664d1",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "## è¯å…ƒåŒ– Tokenize\n",
    "\n",
    "ä¸‹é¢çš„`tokenize`å‡½æ•°å°†æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆ`lines`ï¼‰ä½œä¸ºè¾“å…¥ï¼Œ\n",
    "åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ˆå¦‚ä¸€æ¡æ–‡æœ¬è¡Œï¼‰ã€‚\n",
    "[**æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨**]ï¼Œ*è¯å…ƒ*ï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚\n",
    "æœ€åï¼Œè¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd6a9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.308604Z",
     "iopub.status.busy": "2023-08-18T07:02:26.308048Z",
     "iopub.status.idle": "2023-08-18T07:02:26.317083Z",
     "shell.execute_reply": "2023-08-18T07:02:26.316264Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa119b55",
   "metadata": {},
   "source": [
    "ä¸ºä»€ä¹ˆæ”¯æŒä¸¤ç§ï¼Ÿ\n",
    "- word-levelï¼šé€‚åˆè¯­ä¹‰ä»»åŠ¡\n",
    "- char-levelï¼šé€‚åˆå°æ•°æ® / RNN æ•™å­¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c06e8",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## è¯è¡¨\n",
    "\n",
    "è¯å…ƒçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ¨¡å‹éœ€è¦çš„è¾“å…¥æ˜¯æ•°å­—ï¼Œå› æ­¤è¿™ç§ç±»å‹ä¸æ–¹ä¾¿æ¨¡å‹ä½¿ç”¨ã€‚\n",
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬[**æ„å»ºä¸€ä¸ªå­—å…¸ï¼Œé€šå¸¸ä¹Ÿå«åš*è¯è¡¨*ï¼ˆvocabularyï¼‰ï¼Œ\n",
    "ç”¨æ¥å°†å­—ç¬¦ä¸²ç±»å‹çš„è¯å…ƒæ˜ å°„åˆ°ä»$0$å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­**]ã€‚\n",
    "æˆ‘ä»¬å…ˆå°†è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ–‡æ¡£åˆå¹¶åœ¨ä¸€èµ·ï¼Œå¯¹å®ƒä»¬çš„å”¯ä¸€è¯å…ƒè¿›è¡Œç»Ÿè®¡ï¼Œ\n",
    "å¾—åˆ°çš„ç»Ÿè®¡ç»“æœç§°ä¹‹ä¸º*è¯­æ–™*ï¼ˆcorpusï¼‰ã€‚\n",
    "ç„¶åæ ¹æ®æ¯ä¸ªå”¯ä¸€è¯å…ƒçš„å‡ºç°é¢‘ç‡ï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªæ•°å­—ç´¢å¼•ã€‚\n",
    "\n",
    "å³ï¼Œvocabæ˜¯tokençš„æŒ‡é’ˆç´¢å¼•åˆ—è¡¨\n",
    "corpusæ˜¯tokençš„è¯é¢‘è¡¨ï¼Œå¹¶è¢«èµ‹äºˆäº†æŒ‡é’ˆç´¢å¼•\n",
    "\n",
    "å¾ˆå°‘å‡ºç°çš„è¯å…ƒé€šå¸¸è¢«ç§»é™¤ï¼Œè¿™å¯ä»¥é™ä½å¤æ‚æ€§ã€‚\n",
    "å¦å¤–ï¼Œè¯­æ–™åº“ä¸­ä¸å­˜åœ¨æˆ–å·²åˆ é™¤çš„ä»»ä½•è¯å…ƒéƒ½å°†æ˜ å°„åˆ°ä¸€ä¸ªç‰¹å®šçš„æœªçŸ¥è¯å…ƒâ€œ&lt;unk&gt;â€ã€‚\n",
    "æˆ‘ä»¬å¯ä»¥é€‰æ‹©å¢åŠ ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºä¿å­˜é‚£äº›è¢«ä¿ç•™çš„è¯å…ƒï¼Œ\n",
    "ä¾‹å¦‚ï¼šå¡«å……è¯å…ƒï¼ˆâ€œ&lt;pad&gt;â€ï¼‰ï¼›\n",
    "åºåˆ—å¼€å§‹è¯å…ƒï¼ˆâ€œ&lt;bos&gt;â€ï¼‰ï¼›\n",
    "åºåˆ—ç»“æŸè¯å…ƒï¼ˆâ€œ&lt;eos&gt;â€ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16db7dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.320587Z",
     "iopub.status.busy": "2023-08-18T07:02:26.320050Z",
     "iopub.status.idle": "2023-08-18T07:02:26.330519Z",
     "shell.execute_reply": "2023-08-18T07:02:26.329736Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"æ–‡æœ¬è¯è¡¨\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = [] \n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = [] \n",
    "        # æŒ‰å‡ºç°é¢‘ç‡æ’åº\n",
    "        counter = count_corpus(tokens) # tokenè®¡æ•°å™¨\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡\"\"\"\n",
    "    # è¿™é‡Œçš„tokensæ˜¯1Dåˆ—è¡¨æˆ–2Dåˆ—è¡¨\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä¸€ä¸ªåˆ—è¡¨\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fde2e0",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ—¶å…‰æœºå™¨æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“æ¥[**æ„å»ºè¯è¡¨**]ï¼Œç„¶åæ‰“å°å‰å‡ ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1501d478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.333942Z",
     "iopub.status.busy": "2023-08-18T07:02:26.333382Z",
     "iopub.status.idle": "2023-08-18T07:02:26.346927Z",
     "shell.execute_reply": "2023-08-18T07:02:26.346182Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b78a3",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥(**å°†æ¯ä¸€æ¡æ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨**)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0244f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.350343Z",
     "iopub.status.busy": "2023-08-18T07:02:26.349779Z",
     "iopub.status.idle": "2023-08-18T07:02:26.354215Z",
     "shell.execute_reply": "2023-08-18T07:02:26.353468Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "ç´¢å¼•: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "æ–‡æœ¬: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "ç´¢å¼•: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('æ–‡æœ¬:', tokens[i])\n",
    "    print('ç´¢å¼•:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c1a2a",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## æ•´åˆæ‰€æœ‰åŠŸèƒ½\n",
    "\n",
    "åœ¨ä½¿ç”¨ä¸Šè¿°å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬[**å°†æ‰€æœ‰åŠŸèƒ½æ‰“åŒ…åˆ°`load_corpus_time_machine`å‡½æ•°ä¸­**]ï¼Œ\n",
    "è¯¥å‡½æ•°è¿”å›`corpus`ï¼ˆè¯å…ƒç´¢å¼•åˆ—è¡¨ï¼‰å’Œ`vocab`ï¼ˆæ—¶å…‰æœºå™¨è¯­æ–™åº“çš„è¯è¡¨ï¼‰ã€‚\n",
    "æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„æ”¹å˜æ˜¯ï¼š\n",
    "\n",
    "1. ä¸ºäº†ç®€åŒ–åé¢ç« èŠ‚ä¸­çš„è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨å­—ç¬¦ï¼ˆè€Œä¸æ˜¯å•è¯ï¼‰å®ç°æ–‡æœ¬è¯å…ƒåŒ–ï¼›\n",
    "1. æ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œè¿˜å¯èƒ½æ˜¯ä¸€ä¸ªå•è¯ï¼Œå› æ­¤è¿”å›çš„`corpus`ä»…å¤„ç†ä¸ºå•ä¸ªåˆ—è¡¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤šè¯å…ƒåˆ—è¡¨æ„æˆçš„ä¸€ä¸ªåˆ—è¡¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "578ed76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:02:26.357414Z",
     "iopub.status.busy": "2023-08-18T07:02:26.357141Z",
     "iopub.status.idle": "2023-08-18T07:02:26.470812Z",
     "shell.execute_reply": "2023-08-18T07:02:26.470008Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ\n",
    "    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)\n",
    "# corpusè¯´æ˜è¿™ä¸ªè¡¨é‡Œé¢æœ‰170580ä¸ªä¸åŒçš„è¯\n",
    "# vocab è¯­æ–™è¯è¡¨è¯´æ˜è¿™é‡Œé¢æœ‰26ä¸ªå­—æ¯+ç©ºæ ¼+<unk>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28620a4d",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## å°ç»“\n",
    "\n",
    "* æ–‡æœ¬æ˜¯åºåˆ—æ•°æ®çš„ä¸€ç§æœ€å¸¸è§çš„å½¢å¼ä¹‹ä¸€ã€‚\n",
    "* ä¸ºäº†å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œæˆ‘ä»¬é€šå¸¸å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯å…ƒï¼Œæ„å»ºè¯è¡¨å°†è¯å…ƒå­—ç¬¦ä¸²æ˜ å°„ä¸ºæ•°å­—ç´¢å¼•ï¼Œå¹¶å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºè¯å…ƒç´¢å¼•ä»¥ä¾›æ¨¡å‹æ“ä½œã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137894f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "âœ… ä¸€ã€`vocab` æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "> **`vocab` æ˜¯ä¸€ä¸ªã€Œæ˜ å°„ç®¡ç†å™¨ã€ï¼Œå®ƒæœ¬èº«æ—¢ä¸æ˜¯æ–‡æœ¬è¯­æ–™åº“ï¼Œä¹Ÿä¸æ˜¯ç´¢å¼•è¯­æ–™åº“ï¼Œè€Œæ˜¯ä¸€ä¸ªã€ŒåŒå‘å­—å…¸ã€**ã€‚\n",
    "\n",
    "å…·ä½“æ¥è¯´ï¼š\n",
    "- **å†…éƒ¨å­˜å‚¨äº†ä¸¤æ ·ä¸œè¥¿**ï¼š\n",
    "  1. `idx_to_token`: **åˆ—è¡¨**ï¼ˆlistï¼‰ â†’ **ç´¢å¼• â†’ token**ï¼ˆå¦‚ `[0: '<unk>', 1: 'a', 2: 'b', ...]`ï¼‰\n",
    "  2. `token_to_idx`: **å­—å…¸**ï¼ˆdictï¼‰ â†’ **token â†’ ç´¢å¼•**ï¼ˆå¦‚ `{'a': 1, 'b': 2, ...}`ï¼‰\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "- âœ… `vocab` **åŒ…å«æ–‡æœ¬å½¢å¼çš„ token**ï¼ˆå¦‚ `'a'`, `' '`ï¼‰\n",
    "- âœ… `vocab` **ä¹ŸåŒ…å«å¯¹åº”çš„æ•´æ•°ç´¢å¼•**ï¼ˆå¦‚ `1`, `2`ï¼‰\n",
    "- âŒ ä½†å®ƒ**ä¸æ˜¯è¯­æ–™åº“**ï¼ˆcorpusï¼‰â€”â€”å®ƒä¸ä¿å­˜åŸå§‹æ–‡æœ¬é¡ºåºï¼Œåªä¿å­˜â€œæœ‰å“ªäº› tokenâ€åŠå…¶æ˜ å°„\n",
    "\n",
    "> ğŸ“Œ **ç±»æ¯”**ï¼š  \n",
    "> `vocab` å°±åƒä¸€æœ¬**å­—å…¸**ï¼ˆdictionaryï¼‰ï¼Œå‘Šè¯‰ä½ â€œæ¯ä¸ªå­—å¯¹åº”å“ªä¸ªç¼–å·â€ï¼Œä½†å®ƒä¸æ˜¯â€œæ–‡ç« â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ… äºŒã€`corpus` æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "> **`corpus` æ˜¯ä¸€ä¸ªã€Œæ•´æ•°åˆ—è¡¨ã€ï¼Œè¡¨ç¤ºåŸå§‹æ–‡æœ¬ä¸­æ¯ä¸ª token æŒ‰é¡ºåºå¯¹åº”çš„ç´¢å¼• ID**ã€‚\n",
    "\n",
    "å…·ä½“æ¥è¯´ï¼š\n",
    "- å®ƒæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ç”Ÿæˆçš„ï¼š\n",
    "  ```python\n",
    "  corpus = [vocab[token] for line in tokens for token in line]\n",
    "  ```\n",
    "- å‡è®¾åŸæ–‡æœ¬æ˜¯ `\"ab a\"`ï¼Œåˆ†è¯åä¸º `['a','b',' ','a']`\n",
    "- è‹¥ `vocab['a']=1`, `vocab['b']=2`, `vocab[' ']=3`\n",
    "- åˆ™ `corpus = [1, 2, 3, 1]`\n",
    "\n",
    "æ‰€ä»¥ï¼š\n",
    "- âœ… `corpus` æ˜¯**ç´¢å¼•æ ¼å¼**ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰\n",
    "- âœ… å®ƒ**ä¿ç•™åŸå§‹æ–‡æœ¬çš„é¡ºåºå’Œé‡å¤**\n",
    "- âŒ å®ƒ**ä¸æ˜¯è¯é¢‘åº“**ï¼ˆä¸ç»Ÿè®¡å‡ºç°æ¬¡æ•°ï¼‰\n",
    "- âŒ å®ƒ**ä¸æ˜¯å»é‡çš„é›†åˆ**ï¼ˆåŒä¸€ä¸ª token å¯å‡ºç°å¤šæ¬¡ï¼‰\n",
    "\n",
    "> ğŸ“Œ **å…³é”®ç‚¹**ï¼š  \n",
    "> `corpus` æ˜¯**æ¨¡å‹çš„å®é™…è¾“å…¥æ•°æ®**ï¼Œé•¿åº¦ = åŸå§‹æ–‡æœ¬çš„ token æ€»æ•°ï¼ˆå«é‡å¤ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ” ä¸‰ã€å¯¹æ¯”æ€»ç»“\n",
    "\n",
    "| é¡¹ç›® | ç±»å‹ | å†…å®¹ | æ˜¯å¦å»é‡ï¼Ÿ | æ˜¯å¦æœ‰åºï¼Ÿ | ç”¨é€” |\n",
    "|------|------|------|-----------|-----------|------|\n",
    "| **`vocab`** | **æ˜ å°„å¯¹è±¡**ï¼ˆVocab ç±»å®ä¾‹ï¼‰ | å­˜å‚¨ `token â†” index` çš„åŒå‘æ˜ å°„ | âœ… æ˜¯ï¼ˆæ¯ä¸ª token åªå­˜ä¸€æ¬¡ï¼‰ | âš ï¸ æŒ‰é¢‘æ¬¡æ’åºï¼ˆéæ–‡æœ¬é¡ºåºï¼‰ | æŸ¥è¯¢ ID / è½¬å›æ–‡æœ¬ |\n",
    "| **`corpus`** | **æ•´æ•°åˆ—è¡¨**ï¼ˆlist[int]ï¼‰ | åŸå§‹æ–‡æœ¬ä¸­æ¯ä¸ª token å¯¹åº”çš„ ID åºåˆ— | âŒ å¦ï¼ˆå®Œå…¨ä¿ç•™é‡å¤ï¼‰ | âœ… æ˜¯ï¼ˆä¸¥æ ¼æŒ‰åŸæ–‡é¡ºåºï¼‰ | æ¨¡å‹è®­ç»ƒè¾“å…¥ |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ§ª å››ã€ä¸¾ä¾‹è¯´æ˜\n",
    "\n",
    "å‡è®¾æ¸…æ´—åçš„æ–‡æœ¬åªæœ‰ä¸€è¡Œï¼š\n",
    "```text\n",
    "\"aa b\"\n",
    "```\n",
    "\n",
    "æ­¥éª¤ï¼š\n",
    "1. `tokenize(..., 'char')` â†’ `[['a','a',' ','b']]`\n",
    "2. `Vocab` æ„å»ºåï¼š\n",
    "   - `idx_to_token = ['<unk>', 'a', ' ', 'b']`\n",
    "   - `token_to_idx = {'<unk>':0, 'a':1, ' ':2, 'b':3}`\n",
    "3. `corpus = [vocab['a'], vocab['a'], vocab[' '], vocab['b']] = [1, 1, 2, 3]`\n",
    "\n",
    "ç»“æœï¼š\n",
    "- `vocab` åŒ…å« 4 ä¸ªå”¯ä¸€ tokenï¼ˆå« `<unk>`ï¼‰\n",
    "- `corpus` æ˜¯é•¿åº¦ä¸º 4 çš„æ•´æ•°åˆ—è¡¨ï¼Œ**ä¿ç•™é‡å¤å’Œé¡ºåº**\n",
    "\n",
    "---\n",
    "\n",
    "âœ… æœ€ç»ˆå›ç­”ä½ çš„é—®é¢˜ï¼š\n",
    "\n",
    "> **Q1**: `vocab` æ˜¯æ–‡æœ¬æ ¼å¼çš„è¯­æ–™åº“è¿˜æ˜¯ç´¢å¼•æ ¼å¼çš„è¯­æ–™åº“ï¼Ÿ  \n",
    "> **A1**: **éƒ½ä¸æ˜¯**ã€‚`vocab` æ˜¯ä¸€ä¸ª**æ˜ å°„å·¥å…·**ï¼Œå®ƒå†…éƒ¨åŒæ—¶åŒ…å«**æ–‡æœ¬ token** å’Œ**å¯¹åº”ç´¢å¼•**ï¼Œä½†æœ¬èº«ä¸æ˜¯è¯­æ–™åº“ã€‚\n",
    "\n",
    "> **Q2**: `corpus` æ˜¯æ¯ä¸ª token åªå‡ºç°ä¸€æ¬¡çš„è¯é¢‘åº“è¿˜æ˜¯è¯é¢‘çš„ç´¢å¼•åº“ï¼Ÿ  \n",
    "> **A2**: **éƒ½ä¸æ˜¯**ã€‚`corpus` æ˜¯**åŸå§‹æ–‡æœ¬çš„ç´¢å¼•åºåˆ—**ï¼Œ**ä¿ç•™æ‰€æœ‰é‡å¤å’Œé¡ºåº**ï¼Œ**ä¸ç»Ÿè®¡è¯é¢‘**ï¼Œä¹Ÿ**ä¸å»é‡**ã€‚\n",
    "ğŸ’¡ é™„åŠ è¯´æ˜ï¼šè¯é¢‘åœ¨å“ªé‡Œï¼Ÿ\n",
    "\n",
    "è¯é¢‘ä¿¡æ¯å­˜å‚¨åœ¨ `vocab._token_freqs` ä¸­ï¼Œä¾‹å¦‚ï¼š\n",
    "```python\n",
    "[('a', 2), (' ', 1), ('b', 1)]\n",
    "```\n",
    "è¿™æ‰æ˜¯çœŸæ­£çš„**è¯é¢‘ç»Ÿè®¡ç»“æœ**ï¼Œç”¨äºæ„å»ºè¯è¡¨ï¼ˆå¦‚è¿‡æ»¤ä½é¢‘è¯ï¼‰ã€‚\n",
    "\n",
    "ä½ ç°åœ¨å¯ä»¥æ¸…æ™°åŒºåˆ†ï¼š\n",
    "- **`vocab` = å­—å…¸**ï¼ˆæŸ¥ç¼–å·ï¼‰\n",
    "- **`corpus` = æ–‡ç« çš„ç¼–å·ç‰ˆ**ï¼ˆå–‚ç»™æ¨¡å‹ï¼‰\n",
    "- **`_token_freqs` = è¯é¢‘è¡¨**ï¼ˆç”¨äºåˆ†æå’Œè£å‰ªï¼‰\n",
    "\n",
    "è¿™æ˜¯ NLP æ•°æ® pipeline çš„é»„é‡‘ä¸‰è§’ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4cc025",
   "metadata": {},
   "source": [
    "\n",
    "## ç»ƒä¹ \n",
    "\n",
    "1. è¯å…ƒåŒ–æ˜¯ä¸€ä¸ªå…³é”®çš„é¢„å¤„ç†æ­¥éª¤ï¼Œå®ƒå› è¯­è¨€è€Œå¼‚ã€‚å°è¯•æ‰¾åˆ°å¦å¤–ä¸‰ç§å¸¸ç”¨çš„è¯å…ƒåŒ–æ–‡æœ¬çš„æ–¹æ³•ã€‚\n",
    "1. åœ¨æœ¬èŠ‚çš„å®éªŒä¸­ï¼Œå°†æ–‡æœ¬è¯å…ƒä¸ºå•è¯å’Œæ›´æ”¹`Vocab`å®ä¾‹çš„`min_freq`å‚æ•°ã€‚è¿™å¯¹è¯è¡¨å¤§å°æœ‰ä½•å½±å“ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3b26f",
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2094)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-d2l",
   "language": "python",
   "name": "d2l"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
